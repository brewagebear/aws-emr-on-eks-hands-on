#!/bin/bash

# ê°„ë‹¨í•œ EMR Spark on EKS ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ (MWAA ì œì™¸)
PROJECT_NAME="spark-on-eks-hands-on"
REGION="ap-northeast-2"

echo "ğŸš€ ê°„ë‹¨í•œ EMR Spark on EKS ë°°í¬ (MWAA ì—†ìŒ)"
echo "============================================"

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}â„¹ï¸  $1${NC}"; }
log_success() { echo -e "${GREEN}âœ… $1${NC}"; }
log_warning() { echo -e "${YELLOW}âš ï¸  $1${NC}"; }
log_error() { echo -e "${RED}âŒ $1${NC}"; }

# ì—ëŸ¬ í•¸ë“¤ë§
set -e
trap 'log_error "ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë¼ì¸ $LINENOì—ì„œ ì¤‘ë‹¨ë¨"' ERR

# ì‚¬ì „ ê²€ì¦
check_prerequisites() {
    log_info "ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì¤‘..."

    if ! command -v aws &> /dev/null; then
        log_error "AWS CLIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        echo "ì„¤ì¹˜ ë°©ë²•: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html"
        exit 1
    fi

    if ! aws sts get-caller-identity &> /dev/null; then
        log_error "AWS ìê²© ì¦ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        echo "ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”: aws configure"
        exit 1
    fi

    ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
    log_success "AWS CLI ë° ìê²© ì¦ëª… í™•ì¸ ì™„ë£Œ (ê³„ì •: $ACCOUNT_ID)"
}

# CloudFormation í…œí”Œë¦¿ ê²€ì¦
validate_template() {
    log_info "CloudFormation í…œí”Œë¦¿ ê²€ì¦ ì¤‘..."

    if [ ! -f "emr-spark-eks-infrastructure.yaml" ]; then
        log_error "CloudFormation í…œí”Œë¦¿ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: emr-spark-eks-infrastructure.yaml"
        exit 1
    fi

    aws cloudformation validate-template \
        --template-body file://emr-spark-eks-infrastructure.yaml \
        --region $REGION > /dev/null

    log_success "CloudFormation í…œí”Œë¦¿ ìœ íš¨ì„± ê²€ì‚¬ í†µê³¼"
}

# CloudFormation ìŠ¤íƒ ë°°í¬
deploy_stack() {
    log_info "CloudFormation ìŠ¤íƒ ë°°í¬ ì‹œì‘..."

    # ê¸°ì¡´ ìŠ¤íƒ í™•ì¸
    if aws cloudformation describe-stacks --stack-name $PROJECT_NAME --region $REGION &> /dev/null; then
        log_warning "ê¸°ì¡´ ìŠ¤íƒì´ ì¡´ì¬í•©ë‹ˆë‹¤."
        read -p "ê¸°ì¡´ ìŠ¤íƒì„ ì—…ë°ì´íŠ¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): " update_choice

        if [[ $update_choice =~ ^[Yy]$ ]]; then
            log_info "ìŠ¤íƒ ì—…ë°ì´íŠ¸ ì¤‘..."
            aws cloudformation update-stack \
                --stack-name $PROJECT_NAME \
                --template-body file://emr-spark-eks-infrastructure.yaml \
                --parameters ParameterKey=ProjectName,ParameterValue=$PROJECT_NAME \
                --capabilities CAPABILITY_NAMED_IAM \
                --region $REGION

            log_info "ìŠ¤íƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ ëŒ€ê¸° ì¤‘..."
            aws cloudformation wait stack-update-complete \
                --stack-name $PROJECT_NAME \
                --region $REGION
        else
            log_error "ë°°í¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤."
            exit 1
        fi
    else
        log_info "ìƒˆë¡œìš´ CloudFormation ìŠ¤íƒ ìƒì„± ì¤‘..."
        aws cloudformation create-stack \
            --stack-name $PROJECT_NAME \
            --template-body file://emr-spark-eks-infrastructure.yaml \
            --parameters ParameterKey=ProjectName,ParameterValue=$PROJECT_NAME \
            --capabilities CAPABILITY_NAMED_IAM \
            --region $REGION \
            --on-failure DELETE

        log_info "ìŠ¤íƒ ìƒì„± ì¤‘... (ì•½ 15-20ë¶„ ì†Œìš”)"
        monitor_stack_creation
    fi

    log_success "CloudFormation ìŠ¤íƒ ë°°í¬ ì™„ë£Œ"
}

# ìŠ¤íƒ ìƒì„± ëª¨ë‹ˆí„°ë§
monitor_stack_creation() {
    local start_time=$(date +%s)
    local last_status=""

    while true; do
        stack_status=$(aws cloudformation describe-stacks \
            --stack-name $PROJECT_NAME \
            --query 'Stacks[0].StackStatus' \
            --output text \
            --region $REGION 2>/dev/null)

        if [ "$stack_status" != "$last_status" ]; then
            current_time=$(date +%s)
            elapsed=$((current_time - start_time))
            minutes=$((elapsed / 60))
            echo "   â³ ${minutes}ë¶„ ê²½ê³¼ - ìƒíƒœ: $stack_status"
            last_status="$stack_status"
        fi

        case $stack_status in
            "CREATE_COMPLETE")
                log_success "CloudFormation ìŠ¤íƒ ìƒì„± ì™„ë£Œ"
                return 0
                ;;
            "CREATE_FAILED"|"ROLLBACK_COMPLETE"|"ROLLBACK_FAILED")
                log_error "ìŠ¤íƒ ìƒì„± ì‹¤íŒ¨: $stack_status"
                echo ""
                echo "ì‹¤íŒ¨ ì›ì¸:"
                aws cloudformation describe-stack-events \
                    --stack-name $PROJECT_NAME \
                    --query 'StackEvents[?ResourceStatus==`CREATE_FAILED`].{Resource:LogicalResourceId,Reason:ResourceStatusReason}' \
                    --output table \
                    --region $REGION
                return 1
                ;;
            "CREATE_IN_PROGRESS")
                sleep 30
                ;;
            *)
                log_warning "ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ: $stack_status"
                sleep 30
                ;;
        esac
    done
}

# ìŠ¤íƒ ì¶œë ¥ê°’ ìˆ˜ì§‘
collect_outputs() {
    log_info "ìŠ¤íƒ ì¶œë ¥ê°’ ìˆ˜ì§‘ ì¤‘..."

    DATA_BUCKET=$(aws cloudformation describe-stacks \
        --stack-name $PROJECT_NAME \
        --query 'Stacks[0].Outputs[?OutputKey==`DataBucketName`].OutputValue' \
        --output text \
        --region $REGION)

    LOGS_BUCKET=$(aws cloudformation describe-stacks \
        --stack-name $PROJECT_NAME \
        --query 'Stacks[0].Outputs[?OutputKey==`LogsBucketName`].OutputValue' \
        --output text \
        --region $REGION)

    EMR_ROLE_ARN=$(aws cloudformation describe-stacks \
        --stack-name $PROJECT_NAME \
        --query 'Stacks[0].Outputs[?OutputKey==`EMRExecutionRoleArn`].OutputValue' \
        --output text \
        --region $REGION)

    CLUSTER_NAME=$(aws cloudformation describe-stacks \
        --stack-name $PROJECT_NAME \
        --query 'Stacks[0].Outputs[?OutputKey==`EKSClusterName`].OutputValue' \
        --output text \
        --region $REGION)

    log_success "ìŠ¤íƒ ì¶œë ¥ê°’ ìˆ˜ì§‘ ì™„ë£Œ"
    echo "   ë°ì´í„° ë²„í‚·: $DATA_BUCKET"
    echo "   ë¡œê·¸ ë²„í‚·: $LOGS_BUCKET"
    echo "   EKS í´ëŸ¬ìŠ¤í„°: $CLUSTER_NAME"
}

# ìƒ˜í”Œ ë°ì´í„° ë° Spark ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
create_sample_files() {
    log_info "ìƒ˜í”Œ ë°ì´í„° ë° Spark ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘..."

    # ìƒ˜í”Œ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸
    cat > generate_test_logs.py << 'EOF'
#!/usr/bin/env python3
"""
í…ŒìŠ¤íŠ¸ìš© ì›¹ ë¡œê·¸ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸
"""
import pandas as pd
import random
import boto3
import sys
from datetime import datetime, timedelta
import io

def generate_web_logs(num_records=10000):
    """ì›¹ ë¡œê·¸ ë°ì´í„° ìƒì„±"""
    print(f"ğŸ“Š {num_records}ê°œì˜ í…ŒìŠ¤íŠ¸ ë¡œê·¸ ìƒì„± ì¤‘...")

    # ìƒ˜í”Œ ë°ì´í„° êµ¬ì„± ìš”ì†Œ
    ips = [f"192.168.{random.randint(1,10)}.{random.randint(1,254)}" for _ in range(100)]
    methods = ['GET', 'POST', 'PUT', 'DELETE']
    urls = [f'/api/v1/{endpoint}' for endpoint in ['users', 'orders', 'products', 'analytics', 'reports']]
    status_codes = [200, 201, 400, 404, 500, 502]
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        'Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X)',
        'PostmanRuntime/7.28.4'
    ]

    # ë¡œê·¸ ë°ì´í„° ìƒì„±
    data = []
    base_time = datetime.now() - timedelta(hours=24)

    for i in range(num_records):
        # ì‹œê°„ ë¶„í¬ë¥¼ í˜„ì‹¤ì ìœ¼ë¡œ (í”¼í¬ ì‹œê°„ëŒ€ ê³ ë ¤)
        hour_offset = random.randint(0, 1440)  # 24ì‹œê°„ = 1440ë¶„
        timestamp = base_time + timedelta(minutes=hour_offset)

        # ìƒíƒœ ì½”ë“œë³„ ê°€ì¤‘ì¹˜ (ì„±ê³µì´ ë” ë§ìŒ)
        status_weights = [0.7, 0.1, 0.1, 0.05, 0.03, 0.02]
        status_code = random.choices(status_codes, weights=status_weights)[0]

        # ì‘ë‹µ ì‹œê°„ (ìƒíƒœ ì½”ë“œì— ë”°ë¼ ì¡°ì •)
        if status_code == 200:
            response_time = random.randint(50, 500)
        elif status_code in [400, 404]:
            response_time = random.randint(20, 200)
        else:
            response_time = random.randint(1000, 5000)

        record = {
            'timestamp': timestamp.strftime('%Y-%m-%d %H:%M:%S'),
            'ip_address': random.choice(ips),
            'method': random.choice(methods),
            'url': random.choice(urls),
            'status_code': status_code,
            'response_time_ms': response_time,
            'user_agent': random.choice(user_agents),
            'bytes_sent': random.randint(512, 102400),
            'session_id': f"sess_{random.randint(100000, 999999)}"
        }
        data.append(record)

    return pd.DataFrame(data)

def upload_to_s3(df, bucket_name):
    """DataFrameì„ S3ì— CSVë¡œ ì—…ë¡œë“œ"""
    print(f"ğŸ“¤ S3 ë²„í‚·ì— ì—…ë¡œë“œ ì¤‘: {bucket_name}")

    s3_client = boto3.client('s3')

    # CSV ë³€í™˜
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False)

    # íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    key = f"input/web_logs_{timestamp}.csv"

    # S3 ì—…ë¡œë“œ
    s3_client.put_object(
        Bucket=bucket_name,
        Key=key,
        Body=csv_buffer.getvalue(),
        ContentType='text/csv'
    )

    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: s3://{bucket_name}/{key}")
    return f"s3://{bucket_name}/{key}"

if __name__ == "__main__":
    # ì¸ì íŒŒì‹±
    num_records = int(sys.argv[1]) if len(sys.argv) > 1 else 10000
    bucket_name = sys.argv[2] if len(sys.argv) > 2 else "spark-on-eks-demo-data-ACCOUNT_ID"

    print(f"ğŸ”„ í…ŒìŠ¤íŠ¸ ë¡œê·¸ ìƒì„± ì‹œì‘ (ë ˆì½”ë“œ ìˆ˜: {num_records})")

    # ë°ì´í„° ìƒì„±
    df = generate_web_logs(num_records)

    # ë¡œì»¬ ì €ì¥
    local_file = f"test_web_logs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    df.to_csv(local_file, index=False)
    print(f"ğŸ“ ë¡œì»¬ íŒŒì¼ ì €ì¥: {local_file}")

    # S3 ì—…ë¡œë“œ (ë²„í‚·ì´ ì œê³µëœ ê²½ìš°)
    if len(sys.argv) > 2:
        try:
            s3_path = upload_to_s3(df, bucket_name)
            print(f"ğŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
            print(f"   ë¡œì»¬: {local_file}")
            print(f"   S3: {s3_path}")
        except Exception as e:
            print(f"âš ï¸  S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"ë¡œì»¬ íŒŒì¼ì€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {local_file}")
    else:
        print(f"ğŸ“ ì‚¬ìš©ë²•: python3 {sys.argv[0]} [ë ˆì½”ë“œìˆ˜] [S3ë²„í‚·ëª…]")
        print(f"ë¡œì»¬ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {local_file}")
EOF

    # Spark ë¡œê·¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
    cat > log_analysis_spark.py << 'EOF'
#!/usr/bin/env python3
"""
EMR on EKSì—ì„œ ì‹¤í–‰ë  Spark ë¡œê·¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
"""
import argparse
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, count, count_distinct, sum as spark_sum, avg, max as spark_max, min as spark_min,
    window, to_timestamp, when, desc, hour, date_format, regexp_extract
)
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType
import sys

def create_spark_session():
    """Spark Session ìƒì„±"""
    spark = SparkSession.builder \
        .appName("WebLogAnalysis") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("INFO")
    print("âœ… Spark Session ìƒì„± ì™„ë£Œ")
    return spark

def load_data(spark, input_path):
    """ì›¹ ë¡œê·¸ ë°ì´í„° ë¡œë“œ"""
    print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì‹œì‘: {input_path}")

    # ìŠ¤í‚¤ë§ˆ ì •ì˜
    schema = StructType([
        StructField("timestamp", StringType(), True),
        StructField("ip_address", StringType(), True),
        StructField("method", StringType(), True),
        StructField("url", StringType(), True),
        StructField("status_code", IntegerType(), True),
        StructField("response_time_ms", IntegerType(), True),
        StructField("user_agent", StringType(), True),
        StructField("bytes_sent", IntegerType(), True),
        StructField("session_id", StringType(), True)
    ])

    # CSV íŒŒì¼ ì½ê¸°
    df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "false") \
        .schema(schema) \
        .csv(input_path)

    # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
    df = df.withColumn("timestamp", to_timestamp(col("timestamp"), "yyyy-MM-dd HH:mm:ss"))

    # ë°ì´í„° í’ˆì§ˆ í™•ì¸
    total_records = df.count()
    valid_records = df.filter(col("timestamp").isNotNull()).count()

    print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"   ì´ ë ˆì½”ë“œ: {total_records:,}")
    print(f"   ìœ íš¨ ë ˆì½”ë“œ: {valid_records:,}")

    if valid_records == 0:
        raise ValueError("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")

    # ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥
    print("ğŸ“‹ ë°ì´í„° ìƒ˜í”Œ:")
    df.show(5, truncate=False)

    return df.filter(col("timestamp").isNotNull())

def analyze_traffic_patterns(df):
    """íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„"""
    print("ğŸ“ˆ ì‹œê°„ëŒ€ë³„ íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„ ì¤‘...")

    # ì‹œê°„ëŒ€ë³„ ìš”ì²­ ìˆ˜
    hourly_traffic = df.withColumn("hour", hour("timestamp")) \
        .groupBy("hour") \
        .agg(
            count("*").alias("request_count"),
            avg("response_time_ms").alias("avg_response_time"),
            spark_sum("bytes_sent").alias("total_bytes")
        ) \
        .orderBy("hour")

    print("â° ì‹œê°„ëŒ€ë³„ íŠ¸ë˜í”½:")
    hourly_traffic.show(24)

    return hourly_traffic

def analyze_status_codes(df):
    """HTTP ìƒíƒœ ì½”ë“œ ë¶„ì„"""
    print("ğŸ” HTTP ìƒíƒœ ì½”ë“œ ë¶„ì„ ì¤‘...")

    status_analysis = df.groupBy("status_code") \
        .agg(
            count("*").alias("count"),
            (count("*") * 100.0 / df.count()).alias("percentage"),
            avg("response_time_ms").alias("avg_response_time")
        ) \
        .orderBy(desc("count"))

    print("ğŸ“Š ìƒíƒœ ì½”ë“œë³„ í†µê³„:")
    status_analysis.show()

    return status_analysis

def analyze_endpoints(df):
    """API ì—”ë“œí¬ì¸íŠ¸ ë¶„ì„"""
    print("ğŸ›¤ï¸  API ì—”ë“œí¬ì¸íŠ¸ ë¶„ì„ ì¤‘...")

    endpoint_analysis = df.groupBy("url", "method") \
        .agg(
            count("*").alias("hit_count"),
            avg("response_time_ms").alias("avg_response_time"),
            spark_max("response_time_ms").alias("max_response_time"),
            count(when(col("status_code") >= 400, 1)).alias("error_count")
        ) \
        .withColumn("error_rate", col("error_count") * 100.0 / col("hit_count")) \
        .orderBy(desc("hit_count"))

    print("ğŸ¯ ì—”ë“œí¬ì¸íŠ¸ë³„ í†µê³„:")
    endpoint_analysis.show(10, truncate=False)

    return endpoint_analysis

def analyze_performance(df):
    """ì„±ëŠ¥ ë¶„ì„"""
    print("âš¡ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„ ì¤‘...")

    # ì‘ë‹µ ì‹œê°„ ë¶„í¬
    performance_stats = df.agg(
        avg("response_time_ms").alias("avg_response_time"),
        spark_min("response_time_ms").alias("min_response_time"),
        spark_max("response_time_ms").alias("max_response_time"),
        count("*").alias("total_requests")
    )

    print("ğŸ“ ì‘ë‹µ ì‹œê°„ í†µê³„:")
    performance_stats.show()

    # ëŠë¦° ìš”ì²­ ë¶„ì„ (95th percentile)
    slow_requests = df.filter(col("response_time_ms") > 1000) \
        .select("timestamp", "method", "url", "status_code", "response_time_ms") \
        .orderBy(desc("response_time_ms")) \
        .limit(10)

    print("ğŸŒ ëŠë¦° ìš”ì²­ TOP 10:")
    slow_requests.show(truncate=False)

    return performance_stats, slow_requests

def save_results(df_dict, output_path):
    """ë¶„ì„ ê²°ê³¼ë¥¼ S3ì— ì €ì¥"""
    print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘: {output_path}")

    for name, df in df_dict.items():
        result_path = f"{output_path}/{name}"
        print(f"   ğŸ“ {name} â†’ {result_path}")

        # ì‘ì€ ë°ì´í„°ëŠ” ë‹¨ì¼ íŒŒí‹°ì…˜ìœ¼ë¡œ ì €ì¥
        if df.count() < 1000:
            df = df.coalesce(1)

        # Parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥
        df.write \
          .mode("overwrite") \
          .option("compression", "snappy") \
          .parquet(result_path)

    print("âœ… ëª¨ë“  ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Spark ì›¹ ë¡œê·¸ ë¶„ì„")
    parser.add_argument("--input", required=True, help="ì…ë ¥ ë°ì´í„° S3 ê²½ë¡œ")
    parser.add_argument("--output", required=True, help="ì¶œë ¥ ê²°ê³¼ S3 ê²½ë¡œ")

    args = parser.parse_args()

    print("ğŸš€ Spark ì›¹ ë¡œê·¸ ë¶„ì„ ì‹œì‘")
    print(f"   ì…ë ¥: {args.input}")
    print(f"   ì¶œë ¥: {args.output}")
    print("=" * 60)

    # Spark Session ìƒì„±
    spark = create_spark_session()

    try:
        # ë°ì´í„° ë¡œë“œ
        df = load_data(spark, args.input)

        # ê°ì¢… ë¶„ì„ ìˆ˜í–‰
        hourly_traffic = analyze_traffic_patterns(df)
        status_analysis = analyze_status_codes(df)
        endpoint_analysis = analyze_endpoints(df)
        performance_stats, slow_requests = analyze_performance(df)

        # ì „ì²´ ìš”ì•½ í†µê³„
        print("ğŸ“‹ ì „ì²´ ìš”ì•½:")
        summary = df.agg(
            count("*").alias("total_requests"),
            count_distinct("ip_address").alias("unique_ips"),
            count_distinct("session_id").alias("unique_sessions"),
            avg("response_time_ms").alias("avg_response_time"),
            spark_sum("bytes_sent").alias("total_bytes_sent")
        )
        summary.show()

        # ê²°ê³¼ ì €ì¥
        results = {
            "hourly_traffic": hourly_traffic,
            "status_analysis": status_analysis,
            "endpoint_analysis": endpoint_analysis,
            "performance_stats": performance_stats,
            "slow_requests": slow_requests,
            "summary": summary
        }

        save_results(results, args.output)

        print("ğŸ‰ ì›¹ ë¡œê·¸ ë¶„ì„ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise e
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
EOF

    # S3ì— ì—…ë¡œë“œ
    aws s3 cp generate_test_logs.py s3://$DATA_BUCKET/scripts/
    aws s3 cp log_analysis_spark.py s3://$DATA_BUCKET/scripts/

    log_success "ìƒ˜í”Œ íŒŒì¼ ìƒì„± ë° ì—…ë¡œë“œ ì™„ë£Œ"
    echo "   - í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ê¸°: s3://$DATA_BUCKET/scripts/generate_test_logs.py"
    echo "   - Spark ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸: s3://$DATA_BUCKET/scripts/log_analysis_spark.py"
}

# EMR on EKS ì‘ì—… ì œì¶œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
create_job_submission_script() {
    log_info "EMR ì‘ì—… ì œì¶œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘..."

    cat > submit_spark_job.py << EOF
#!/usr/bin/env python3
"""
EMR on EKS Spark ì‘ì—… ì œì¶œ ìŠ¤í¬ë¦½íŠ¸
"""
import boto3
import json
import time
import sys
from datetime import datetime

# í™˜ê²½ ì„¤ì •
PROJECT_NAME = "$PROJECT_NAME"
REGION = "$REGION"
DATA_BUCKET = "$DATA_BUCKET"
LOGS_BUCKET = "$LOGS_BUCKET"
EMR_ROLE_ARN = "$EMR_ROLE_ARN"

def get_virtual_cluster_id():
    """Virtual Cluster ID ì°¾ê¸°"""
    emr_client = boto3.client('emr-containers', region_name=REGION)

    try:
        response = emr_client.list_virtual_clusters()
        clusters = [vc for vc in response['virtualClusters']
                   if vc['name'] == f'{PROJECT_NAME}-virtual-cluster' and vc['state'] == 'RUNNING']

        if clusters:
            return clusters[0]['id']
        else:
            print("âŒ ì‹¤í–‰ ì¤‘ì¸ Virtual Clusterë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ë¨¼ì € ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”: ./setup-emr-virtual-cluster.sh")
            return None
    except Exception as e:
        print(f"âŒ Virtual Cluster í™•ì¸ ì‹¤íŒ¨: {e}")
        return None

def submit_spark_job(virtual_cluster_id, input_path, output_path):
    """Spark ì‘ì—… ì œì¶œ"""
    emr_client = boto3.client('emr-containers', region_name=REGION)

    job_name = f"log-analysis-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

    job_config = {
        "name": job_name,
        "virtualClusterId": virtual_cluster_id,
        "executionRoleArn": EMR_ROLE_ARN,
        "releaseLabel": "emr-6.15.0-latest",
        "jobDriver": {
            "sparkSubmitJobDriver": {
                "entryPoint": f"s3://{DATA_BUCKET}/scripts/log_analysis_spark.py",
                "entryPointArguments": [
                    "--input", input_path,
                    "--output", output_path
                ],
                "sparkSubmitParameters": (
                    "--conf spark.executor.instances=2 "
                    "--conf spark.executor.memory=2g "
                    "--conf spark.executor.cores=1 "
                    "--conf spark.driver.memory=2g "
                    "--conf spark.sql.adaptive.enabled=true "
                    "--conf spark.sql.adaptive.coalescePartitions.enabled=true "
                    "--conf spark.kubernetes.executor.deleteOnTermination=true "
                    "--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
                )
            }
        },
        "configurationOverrides": {
            "applicationConfiguration": [
                {
                    "classification": "spark-defaults",
                    "properties": {
                        "spark.sql.adaptive.enabled": "true",
                        "spark.sql.adaptive.coalescePartitions.enabled": "true"
                    }
                }
            ],
            "monitoringConfiguration": {
                "cloudWatchMonitoringConfiguration": {
                    "logGroupName": f"/aws/emr-containers/{PROJECT_NAME}",
                    "logStreamNamePrefix": "spark-job"
                },
                "s3MonitoringConfiguration": {
                    "logUri": f"s3://{LOGS_BUCKET}/spark-logs/"
                }
            }
        }
    }

    print(f"ğŸš€ Spark ì‘ì—… ì œì¶œ ì¤‘: {job_name}")
    print(f"   ì…ë ¥: {input_path}")
    print(f"   ì¶œë ¥: {output_path}")

    try:
        response = emr_client.start_job_run(**job_config)
        job_id = response['id']

        print(f"âœ… ì‘ì—… ì œì¶œ ì„±ê³µ!")
        print(f"   ì‘ì—… ID: {job_id}")
        print(f"   Virtual Cluster: {virtual_cluster_id}")

        return job_id

    except Exception as e:
        print(f"âŒ ì‘ì—… ì œì¶œ ì‹¤íŒ¨: {e}")
        return None

def monitor_job(job_id, virtual_cluster_id):
    """ì‘ì—… ìƒíƒœ ëª¨ë‹ˆí„°ë§"""
    emr_client = boto3.client('emr-containers', region_name=REGION)

    print(f"â³ ì‘ì—… ìƒíƒœ ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
    print("   ìƒíƒœë¥¼ í™•ì¸í•˜ë ¤ë©´ Ctrl+Cë¡œ ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    try:
        while True:
            response = emr_client.describe_job_run(
                virtualClusterId=virtual_cluster_id,
                id=job_id
            )

            job_run = response['jobRun']
            state = job_run['state']

            print(f"   ğŸ“Š ìƒíƒœ: {state}")

            if state == 'COMPLETED':
                print("ğŸ‰ ì‘ì—… ì™„ë£Œ!")
                return True
            elif state in ['FAILED', 'CANCELLED']:
                print(f"âŒ ì‘ì—… ì‹¤íŒ¨: {state}")
                if 'stateDetails' in job_run:
                    print(f"   ì„¸ë¶€ì‚¬í•­: {job_run['stateDetails']}")
                return False

            time.sleep(30)  # 30ì´ˆë§ˆë‹¤ ìƒíƒœ í™•ì¸

    except KeyboardInterrupt:
        print("\\nâ¸ï¸  ëª¨ë‹ˆí„°ë§ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        print(f"ì‘ì—…ì€ ê³„ì† ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ìƒíƒœ í™•ì¸:")
        print(f"aws emr-containers describe-job-run --virtual-cluster-id {virtual_cluster_id} --id {job_id}")
        return None

def main():
    print("ğŸ”¥ EMR on EKS Spark ì‘ì—… ì œì¶œê¸°")
    print("=" * 50)

    # Virtual Cluster í™•ì¸
    virtual_cluster_id = get_virtual_cluster_id()
    if not virtual_cluster_id:
        sys.exit(1)

    # ì…ë ¥ ë°ì´í„° ê²½ë¡œ (ê¸°ë³¸ê°’ ë˜ëŠ” ì¸ìì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    if len(sys.argv) > 1:
        input_path = sys.argv[1]
    else:
        input_path = f"s3://{DATA_BUCKET}/input/"

    # ì¶œë ¥ ê²½ë¡œ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_path = f"s3://{DATA_BUCKET}/output/analysis_{timestamp}"

    print(f"ğŸ“‹ ì‘ì—… ì„¤ì •:")
    print(f"   Virtual Cluster: {virtual_cluster_id}")
    print(f"   ì…ë ¥ ë°ì´í„°: {input_path}")
    print(f"   ì¶œë ¥ ê²½ë¡œ: {output_path}")
    print()

    # ì‘ì—… ì œì¶œ
    job_id = submit_spark_job(virtual_cluster_id, input_path, output_path)

    if job_id:
        print()
        print("ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ìˆ˜ë™ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥:")
        print(f"aws emr-containers describe-job-run --virtual-cluster-id {virtual_cluster_id} --id {job_id}")
        print()

        # ëª¨ë‹ˆí„°ë§ ì—¬ë¶€ í™•ì¸
        monitor_choice = input("ì‘ì—… ìƒíƒœë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if monitor_choice.lower() in ['y', 'yes']:
            monitor_job(job_id, virtual_cluster_id)
        else:
            print("ëª¨ë‹ˆí„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤. ë‚˜ì¤‘ì— ìœ„ ëª…ë ¹ìœ¼ë¡œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()
EOF

    chmod +x submit_spark_job.py
    log_success "EMR ì‘ì—… ì œì¶œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì™„ë£Œ"
}

# ì„¤ì • íŒŒì¼ ë° ë„ì›€ë§ ìƒì„±
create_configuration_files() {
    log_info "ì„¤ì • íŒŒì¼ ë° ë„ì›€ë§ ìƒì„± ì¤‘..."

    # í™˜ê²½ ë³€ìˆ˜ íŒŒì¼
    cat > .env << EOF
# EMR Spark on EKS í™˜ê²½ ë³€ìˆ˜ (MWAA ì œì™¸)
PROJECT_NAME=$PROJECT_NAME
REGION=$REGION
DATA_BUCKET=$DATA_BUCKET
LOGS_BUCKET=$LOGS_BUCKET
EMR_ROLE_ARN=$EMR_ROLE_ARN
CLUSTER_NAME=$CLUSTER_NAME
EOF

    # í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
    cat > test_deployment.py << EOF
#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ ë°°í¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""
import boto3

def test_s3_buckets():
    """S3 ë²„í‚· ì ‘ê·¼ í…ŒìŠ¤íŠ¸"""
    print("ğŸª£ S3 ë²„í‚· í…ŒìŠ¤íŠ¸")
    s3_client = boto3.client('s3', region_name='$REGION')

    buckets = ['$DATA_BUCKET', '$LOGS_BUCKET']

    for bucket in buckets:
        try:
            s3_client.head_bucket(Bucket=bucket)
            print(f"   âœ… {bucket}")
        except Exception as e:
            print(f"   âŒ {bucket}: {e}")

def test_eks_cluster():
    """EKS í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸"""
    print("ğŸš¢ EKS í´ëŸ¬ìŠ¤í„° í…ŒìŠ¤íŠ¸")
    import subprocess

    try:
        result = subprocess.run(['kubectl', 'get', 'nodes'],
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("   âœ… EKS í´ëŸ¬ìŠ¤í„° ì ‘ê·¼ ê°€ëŠ¥")
            node_count = len([line for line in result.stdout.split('\\n')
                            if 'Ready' in line])
            print(f"   ğŸ“Š í™œì„± ë…¸ë“œ ìˆ˜: {node_count}")
        else:
            print("   âŒ EKS í´ëŸ¬ìŠ¤í„° ì ‘ê·¼ ì‹¤íŒ¨")
    except FileNotFoundError:
        print("   âš ï¸  kubectlì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")

def test_emr_virtual_cluster():
    """EMR Virtual Cluster í™•ì¸"""
    print("âš¡ EMR Virtual Cluster í…ŒìŠ¤íŠ¸")
    emr_client = boto3.client('emr-containers', region_name='$REGION')

    try:
        response = emr_client.list_virtual_clusters()
        clusters = [vc for vc in response['virtualClusters']
                   if vc['name'] == '${PROJECT_NAME}-virtual-cluster']

        if clusters:
            cluster = clusters[0]
            print(f"   âœ… Virtual Cluster ìƒíƒœ: {cluster['state']}")
            print(f"   ğŸ“‹ ID: {cluster['id']}")
        else:
            print("   âš ï¸  Virtual Clusterë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            print("   ğŸ’¡ ./setup-emr-virtual-cluster.sh ì‹¤í–‰ í•„ìš”")
    except Exception as e:
        print(f"   âŒ Virtual Cluster í™•ì¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print("ğŸ§ª EMR Spark on EKS ë°°í¬ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    test_s3_buckets()
    print()
    test_eks_cluster()
    print()
    test_emr_virtual_cluster()
    print()
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
EOF

    chmod +x test_deployment.py

    # ì‚¬ìš©ë²• ê°€ì´ë“œ
    cat > QUICK_START.md << 'EOF'
# ğŸš€ EMR Spark on EKS ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## 1. ì¸í”„ë¼ ë°°í¬
```bash
./simple_deploy.sh
```

## 2. EKS ë° EMR Virtual Cluster ì„¤ì •
```bash
./setup-emr-virtual-cluster.sh
```

## 3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
```bash
# 10,000ê°œ ë¡œê·¸ ë ˆì½”ë“œ ìƒì„± ë° S3 ì—…ë¡œë“œ
python3 generate_test_logs.py 10000 YOUR_DATA_BUCKET_NAME
```

## 4. Spark ì‘ì—… ì‹¤í–‰
```bash
# ë¡œê·¸ ë¶„ì„ ì‘ì—… ì œì¶œ
python3 submit_spark_job.py
```

## 5. ê²°ê³¼ í™•ì¸
```bash
# S3ì—ì„œ ê²°ê³¼ í™•ì¸
aws s3 ls s3://YOUR_DATA_BUCKET/output/ --recursive
```

## ğŸ’° ë¹„ìš© ì ˆì•½
```bash
# ì‹¤ìŠµ ì™„ë£Œ í›„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
./cost_optimization.sh
```

## ğŸ” ë¬¸ì œ í•´ê²°
```bash
# ë°°í¬ í…ŒìŠ¤íŠ¸
python3 test_deployment.py

# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
kubectl get nodes
kubectl get pods -n emr

# Virtual Cluster ìƒíƒœ
aws emr-containers list-virtual-clusters
```
EOF

    log_success "ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ"
}

# ì™„ë£Œ ë©”ì‹œì§€
show_completion_message() {
    log_success "ğŸ‰ ê°„ë‹¨í•œ EMR Spark on EKS ë°°í¬ ì™„ë£Œ!"
    echo "============================================"

    echo ""
    echo "ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:"
    echo "1. EKS ë° EMR Virtual Cluster ì„¤ì •:"
    echo "   ./setup-emr-virtual-cluster.sh"
    echo ""
    echo "2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±:"
    echo "   python3 generate_test_logs.py 10000 $DATA_BUCKET"
    echo ""
    echo "3. Spark ì‘ì—… ì‹¤í–‰:"
    echo "   python3 submit_spark_job.py"
    echo ""
    echo "4. ë°°í¬ í…ŒìŠ¤íŠ¸:"
    echo "   python3 test_deployment.py"
    echo ""
    echo "ğŸ“š ìì„¸í•œ ì‚¬ìš©ë²•: cat QUICK_START.md"
    echo ""
    echo "ğŸ’° ì˜ˆìƒ ì‹œê°„ë‹¹ ë¹„ìš© (MWAA ì œì™¸): ~$0.19 (~â‚©250)"
    echo "   - EKS ì œì–´ í‰ë©´: $0.10"
    echo "   - EKS ë…¸ë“œ (t3.medium spot x2): ~$0.03"
    echo "   - NAT Gateway: $0.059"
    echo "   - ê¸°íƒ€ (S3, CloudWatch): ~$0.001"
    echo ""
    echo "âš ï¸  ì‹¤ìŠµ ì™„ë£Œ í›„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬: ./cost_optimization.sh"
}

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
main() {
    echo "ê°„ë‹¨í•œ EMR Spark on EKS ë°°í¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤..."
    echo "MWAA ì—†ì´ ì§ì ‘ Spark ì‘ì—…ì„ ì œì¶œí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤."
    echo ""

    read -p "ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): " confirm

    if [[ ! $confirm =~ ^[Yy]$ ]]; then
        echo "ë°°í¬ë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤."
        exit 0
    fi

    # ë‹¨ê³„ë³„ ì‹¤í–‰
    check_prerequisites
    validate_template
    deploy_stack
    collect_outputs
    create_sample_files
    create_job_submission_script
    create_configuration_files
    show_completion_message
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@"