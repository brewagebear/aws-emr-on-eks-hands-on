#!/bin/bash

# ê°„ë‹¨í•œ EMR Virtual Cluster ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ (MWAA ì—†ëŠ” ë²„ì „)
PROJECT_NAME="spark-on-eks-hands-on"
REGION="ap-northeast-2"
CLUSTER_NAME="${PROJECT_NAME}-cluster"

echo "ğŸš€ EMR Virtual Cluster ë° Spark Operator ì„¤ì • (ê°„ë‹¨í•œ ë²„ì „)"
echo "============================================================"

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}â„¹ï¸  $1${NC}"; }
log_success() { echo -e "${GREEN}âœ… $1${NC}"; }
log_warning() { echo -e "${YELLOW}âš ï¸  $1${NC}"; }
log_error() { echo -e "${RED}âŒ $1${NC}"; }

# ì—ëŸ¬ í•¸ë“¤ë§
set -e
trap 'log_error "ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ë¼ì¸ $LINENOì—ì„œ ì¤‘ë‹¨ë¨"' ERR

# ì‚¬ì „ ê²€ì‚¬
check_prerequisites() {
    log_info "ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸..."

    # í•„ìˆ˜ ë„êµ¬ í™•ì¸
    local missing_tools=()

    for tool in aws kubectl helm; do
        if ! command -v $tool >/dev/null 2>&1; then
            missing_tools+=($tool)
        fi
    done

    if [ ${#missing_tools[@]} -ne 0 ]; then
        log_error "ë‹¤ìŒ ë„êµ¬ë“¤ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: ${missing_tools[*]}"
        echo ""
        echo "ì„¤ì¹˜ ë°©ë²•:"
        echo "AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html"
        echo "kubectl: curl -LO \"https://dl.k8s.io/release/\$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\""
        echo "helm: curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash"
        exit 1
    fi

    # AWS ìê²© ì¦ëª… í™•ì¸
    if ! aws sts get-caller-identity &> /dev/null; then
        log_error "AWS ìê²© ì¦ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        echo "ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”: aws configure"
        exit 1
    fi

    # CloudFormation ìŠ¤íƒ í™•ì¸
    if ! aws cloudformation describe-stacks --stack-name $PROJECT_NAME --region $REGION &> /dev/null; then
        log_error "CloudFormation ìŠ¤íƒì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        echo "ë¨¼ì € ë‹¤ìŒì„ ì‹¤í–‰í•˜ì„¸ìš”: ./simple_deploy.sh"
        exit 1
    fi

    log_success "ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì™„ë£Œ"
}

# EKS í´ëŸ¬ìŠ¤í„° ì—°ê²°
setup_eks_connection() {
    log_info "EKS í´ëŸ¬ìŠ¤í„° ì—°ê²° ì„¤ì •..."

    # kubeconfig ì—…ë°ì´íŠ¸
    aws eks update-kubeconfig --region $REGION --name $CLUSTER_NAME

    # í´ëŸ¬ìŠ¤í„° ì ‘ê·¼ í™•ì¸
    if kubectl get nodes >/dev/null 2>&1; then
        local node_count=$(kubectl get nodes --no-headers | wc -l)
        log_success "EKS í´ëŸ¬ìŠ¤í„° ì—°ê²° ì„±ê³µ (ë…¸ë“œ ìˆ˜: $node_count)"

        echo "í˜„ì¬ ë…¸ë“œ ìƒíƒœ:"
        kubectl get nodes
    else
        log_error "EKS í´ëŸ¬ìŠ¤í„°ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        echo "í´ëŸ¬ìŠ¤í„°ê°€ ì™„ì „íˆ ìƒì„±ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦° í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
        echo "í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸: aws eks describe-cluster --name $CLUSTER_NAME --region $REGION"
        exit 1
    fi
}

# Kubernetes ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë° RBAC ì„¤ì •
setup_kubernetes_resources() {
    log_info "Kubernetes ë¦¬ì†ŒìŠ¤ ì„¤ì •..."

    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
    kubectl create namespace spark-operator --dry-run=true -o yaml | kubectl apply -f -
    kubectl create namespace emr --dry-run=true -o yaml | kubectl apply -f -

    log_success "ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ (spark-operator, emr)"

    # EMR ì‹¤í–‰ ì—­í•  ARN ê°€ì ¸ì˜¤ê¸°
    local EMR_ROLE_ARN=$(aws cloudformation describe-stacks \
        --stack-name $PROJECT_NAME \
        --query 'Stacks[0].Outputs[?OutputKey==`EMRExecutionRoleArn`].OutputValue' \
        --output text \
        --region $REGION)

    log_info "EMR ì„œë¹„ìŠ¤ ê³„ì • ë° RBAC ì„¤ì • ì¤‘..."

    # EMRì„ ìœ„í•œ ì„œë¹„ìŠ¤ ê³„ì • ë° RBAC ì„¤ì •
    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: emr-containers-sa-spark
  namespace: emr
  annotations:
    eks.amazonaws.com/role-arn: $EMR_ROLE_ARN
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: emr
  name: emr-containers-role-spark
rules:
- apiGroups: [""]
  resources: ["pods", "services", "endpoints", "persistentvolumeclaims", "events", "configmaps", "secrets"]
  verbs: ["*"]
- apiGroups: [""]
  resources: ["namespaces"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "daemonsets", "replicasets", "statefulsets"]
  verbs: ["*"]
- apiGroups: ["extensions"]
  resources: ["ingresses"]
  verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: emr-containers-rb-spark
  namespace: emr
subjects:
- kind: ServiceAccount
  name: emr-containers-sa-spark
  namespace: emr
roleRef:
  kind: Role
  name: emr-containers-role-spark
  apiGroup: rbac.authorization.k8s.io
EOF

    log_success "EMR RBAC ì„¤ì • ì™„ë£Œ"
}

# Spark Operator ì„¤ì¹˜
install_spark_operator() {
    log_info "Spark Operator ì„¤ì¹˜..."

    # Helm ë¦¬í¬ì§€í† ë¦¬ ì¶”ê°€
    helm repo add --force-update spark-operator https://kubeflow.github.io/spark-operator

    # ê¸°ì¡´ ì„¤ì¹˜ í™•ì¸
    if helm list  -n spark-operator | grep -q spark-operator; then
        log_warning "Spark Operatorê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì—…ê·¸ë ˆì´ë“œ ì¤‘..."
        helm upgrade spark-operator spark-operator/spark-operator \
            --namespace spark-operator \
            --set webhook.enable=true \
            --set metrics.enable=true \
            --set metrics.port=10254 \
            --set sparkJobNamespace=emr
    else
        helm install spark-operator spark-operator/spark-operator \
            --namespace spark-operator \
            --set webhook.enable=true \
            --set metrics.enable=true \
            --set metrics.port=10254 \
            --set sparkJobNamespace=emr
    fi

    # Spark Operator ìƒíƒœ í™•ì¸
    log_info "Spark Operator ì¤€ë¹„ ëŒ€ê¸° ì¤‘..."
    kubectl wait --for=condition=available deployment \
    -l app.kubernetes.io/name=spark-operator \
    --timeout=300s -n spark-operator

    log_success "Spark Operator ì„¤ì¹˜ ì™„ë£Œ"
}

# EMR Virtual Cluster ìƒì„±
create_emr_virtual_cluster() {
    log_info "EMR Virtual Cluster ìƒì„±..."

    # ê¸°ì¡´ Virtual Cluster í™•ì¸
    local existing_cluster=$(aws emr-containers list-virtual-clusters \
        --query "virtualClusters[?name=='${PROJECT_NAME}-virtual-cluster' && state=='RUNNING'].id" \
        --output text \
        --region $REGION)

    if [ ! -z "$existing_cluster" ] && [ "$existing_cluster" != "None" ]; then
        log_warning "Virtual Clusterê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: $existing_cluster"
        VIRTUAL_CLUSTER_ID=$existing_cluster
    else
        # ìƒˆ Virtual Cluster ìƒì„±
        # EKS API and ConfigMap ì„¤ì •ì„ í•´ì•¼í•©ë‹ˆë‹¤.
        log_info "ìƒˆë¡œìš´ Virtual Cluster ìƒì„± ì¤‘..."
        VIRTUAL_CLUSTER_ID=$(aws emr-containers create-virtual-cluster \
            --name "${PROJECT_NAME}-virtual-cluster" \
            --container-provider '{
                "type": "EKS",
                "id": "'$CLUSTER_NAME'",
                "info": {
                    "eksInfo": {
                        "namespace": "emr"
                    }
                }
            }' \
            --region $REGION \
            --query 'id' \
            --output text)

        # Virtual Cluster ìƒíƒœ í™•ì¸
        log_info "Virtual Cluster ìƒì„± ëŒ€ê¸° ì¤‘..."
        local max_attempts=20
        local attempt=0

        while [ $attempt -lt $max_attempts ]; do
            local cluster_state=$(aws emr-containers describe-virtual-cluster \
                --id $VIRTUAL_CLUSTER_ID \
                --query 'virtualCluster.state' \
                --output text \
                --region $REGION)

            case $cluster_state in
                "RUNNING")
                    log_success "Virtual Cluster ì¤€ë¹„ ì™„ë£Œ"
                    break
                    ;;
                "ARRESTED"|"TERMINATED")
                    log_error "Virtual Cluster ìƒì„± ì‹¤íŒ¨: $cluster_state"
                    exit 1
                    ;;
                *)
                    echo "   â³ Virtual Cluster ìƒíƒœ: $cluster_state (ì‹œë„ $((attempt+1))/$max_attempts)"
                    sleep 15
                    attempt=$((attempt+1))
                    ;;
            esac
        done

        if [ $attempt -eq $max_attempts ]; then
            log_error "Virtual Cluster ìƒì„± íƒ€ì„ì•„ì›ƒ"
            exit 1
        fi
    fi

    log_success "Virtual Cluster ID: $VIRTUAL_CLUSTER_ID"
}

# í…ŒìŠ¤íŠ¸ Spark ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
create_test_spark_app() {
    log_info "í…ŒìŠ¤íŠ¸ Spark ì• í”Œë¦¬ì¼€ì´ì…˜ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±..."

    cat > test-spark-pi.yaml << EOF
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi-test
  namespace: emr
spec:
  type: Scala
  mode: cluster
  image: public.ecr.aws/emr-on-eks/spark/emr-6.15.0:latest
  imagePullPolicy: Always
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///usr/lib/spark/examples/jars/spark-examples.jar
  arguments:
    - "100"
  sparkVersion: "3.4.1"
  restartPolicy:
    type: Never
  driver:
    cores: 1
    coreLimit: "1000m"
    memory: "1g"
    serviceAccount: emr-containers-sa-spark
    labels:
      version: 3.4.1
    env:
      - name: AWS_REGION
        value: "$REGION"
  executor:
    cores: 1
    instances: 2
    memory: "1g"
    labels:
      version: 3.4.1
    env:
      - name: AWS_REGION
        value: "$REGION"
EOF

    log_success "í…ŒìŠ¤íŠ¸ Spark ì• í”Œë¦¬ì¼€ì´ì…˜ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ"
    echo "   íŒŒì¼: test-spark-pi.yaml"
    echo "   í…ŒìŠ¤íŠ¸ ì‹¤í–‰: kubectl apply -f test-spark-pi.yaml"
    echo "   ìƒíƒœ í™•ì¸: kubectl get sparkapplications -n emr"
    echo "   ë¡œê·¸ í™•ì¸: kubectl logs -f [pod-name] -n emr"
}

# í™˜ê²½ ì •ë³´ ì—…ë°ì´íŠ¸
update_environment_info() {
    log_info "í™˜ê²½ ì •ë³´ ì—…ë°ì´íŠ¸..."

    # .env íŒŒì¼ ì—…ë°ì´íŠ¸
    if [ -f ".env" ]; then
        # Virtual Cluster ID ì¶”ê°€
        if grep -q "VIRTUAL_CLUSTER_ID" .env; then
            sed -i "s/VIRTUAL_CLUSTER_ID=.*/VIRTUAL_CLUSTER_ID=$VIRTUAL_CLUSTER_ID/" .env
        else
            echo "VIRTUAL_CLUSTER_ID=$VIRTUAL_CLUSTER_ID" >> .env
        fi
    fi

    # ì—…ë°ì´íŠ¸ëœ ì‘ì—… ì œì¶œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    cat > submit_test_spark_job.py << EOF
#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ Spark ì‘ì—… ì œì¶œ ìŠ¤í¬ë¦½íŠ¸
"""
import boto3
import time
from datetime import datetime

VIRTUAL_CLUSTER_ID = "$VIRTUAL_CLUSTER_ID"
REGION = "$REGION"
PROJECT_NAME = "$PROJECT_NAME"

def submit_pi_calculation():
    """ê°„ë‹¨í•œ Pi ê³„ì‚° ì‘ì—… ì œì¶œ"""
    emr_client = boto3.client('emr-containers', region_name=REGION)

    # EMR ì—­í•  ARN ê°€ì ¸ì˜¤ê¸°
    cf_client = boto3.client('cloudformation', region_name=REGION)
    response = cf_client.describe_stacks(StackName=PROJECT_NAME)

    emr_role_arn = None
    for output in response['Stacks'][0]['Outputs']:
        if output['OutputKey'] == 'EMRExecutionRoleArn':
            emr_role_arn = output['OutputValue']
            break

    if not emr_role_arn:
        print("âŒ EMR ì‹¤í–‰ ì—­í• ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    job_name = f"pi-calculation-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

    job_config = {
        "name": job_name,
        "virtualClusterId": VIRTUAL_CLUSTER_ID,
        "executionRoleArn": emr_role_arn,
        "releaseLabel": "emr-6.15.0-latest",
        "jobDriver": {
            "sparkSubmitJobDriver": {
                "entryPoint": "local:///usr/lib/spark/examples/jars/spark-examples.jar",
                "entryPointArguments": ["100"],
                "sparkSubmitParameters": (
                    "--class org.apache.spark.examples.SparkPi "
                    "--conf spark.executor.instances=2 "
                    "--conf spark.executor.memory=1g "
                    "--conf spark.executor.cores=1 "
                    "--conf spark.driver.memory=1g"
                )
            }
        },
        "configurationOverrides": {
            "monitoringConfiguration": {
                "cloudWatchMonitoringConfiguration": {
                    "logGroupName": f"/aws/emr-containers/{PROJECT_NAME}",
                    "logStreamNamePrefix": "pi-test"
                }
            }
        }
    }

    print(f"ğŸš€ Pi ê³„ì‚° ì‘ì—… ì œì¶œ ì¤‘: {job_name}")

    try:
        response = emr_client.start_job_run(**job_config)
        job_id = response['id']

        print(f"âœ… ì‘ì—… ì œì¶œ ì„±ê³µ!")
        print(f"   ì‘ì—… ID: {job_id}")
        print(f"   Virtual Cluster: {VIRTUAL_CLUSTER_ID}")

        return job_id

    except Exception as e:
        print(f"âŒ ì‘ì—… ì œì¶œ ì‹¤íŒ¨: {e}")
        return None

def check_job_status(job_id):
    """ì‘ì—… ìƒíƒœ í™•ì¸"""
    emr_client = boto3.client('emr-containers', region_name=REGION)

    try:
        response = emr_client.describe_job_run(
            virtualClusterId=VIRTUAL_CLUSTER_ID,
            id=job_id
        )

        job_run = response['jobRun']
        print(f"ğŸ“Š ì‘ì—… ìƒíƒœ: {job_run['state']}")

        if 'stateDetails' in job_run:
            print(f"   ì„¸ë¶€ì‚¬í•­: {job_run['stateDetails']}")

        return job_run['state']

    except Exception as e:
        print(f"âŒ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        return None

if __name__ == "__main__":
    print("ğŸ§® EMR on EKS Pi ê³„ì‚° í…ŒìŠ¤íŠ¸")
    print("=" * 40)

    job_id = submit_pi_calculation()

    if job_id:
        print("\\nì‘ì—… ìƒíƒœ í™•ì¸:")
        print(f"aws emr-containers describe-job-run --virtual-cluster-id {VIRTUAL_CLUSTER_ID} --id {job_id}")

        # ê°„ë‹¨í•œ ìƒíƒœ í™•ì¸
        time.sleep(5)
        check_job_status(job_id)
EOF

    chmod +x submit_test_spark_job.py
    log_success "í™˜ê²½ ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ"
}

# ìƒíƒœ í™•ì¸ ë° ê²€ì¦
verify_setup() {
    log_info "ì„¤ì • ê²€ì¦ ì¤‘..."

    echo "ğŸ“Š í˜„ì¬ ìƒíƒœ:"
    echo ""

    # EKS ë…¸ë“œ ìƒíƒœ
    echo "ğŸš¢ EKS ë…¸ë“œ:"
    kubectl get nodes
    echo ""

    # Spark Operator ìƒíƒœ
    echo "âš¡ Spark Operator:"
    kubectl get pods -n spark-operator
    echo ""

    # EMR ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¦¬ì†ŒìŠ¤
    echo "ğŸ“¦ EMR ë„¤ì„ìŠ¤í˜ì´ìŠ¤:"
    kubectl get all -n emr
    echo ""

    # Virtual Cluster ìƒíƒœ
    echo "ğŸ”— Virtual Cluster:"
    aws emr-containers describe-virtual-cluster \
        --id $VIRTUAL_CLUSTER_ID \
        --query 'virtualCluster.{Name:name,State:state,Id:id}' \
        --output table \
        --region $REGION

    log_success "ì„¤ì • ê²€ì¦ ì™„ë£Œ"
}

# ì™„ë£Œ ë©”ì‹œì§€
show_completion_message() {
    log_success "ğŸ‰ EMR Virtual Cluster ì„¤ì • ì™„ë£Œ!"
    echo "================================================"
    echo ""
    echo "ğŸ“‹ ìƒì„±ëœ ë¦¬ì†ŒìŠ¤:"
    echo "   âœ… EKS í´ëŸ¬ìŠ¤í„°: $CLUSTER_NAME"
    echo "   âœ… Virtual Cluster: $VIRTUAL_CLUSTER_ID"
    echo "   âœ… Spark Operator (ë„¤ì„ìŠ¤í˜ì´ìŠ¤: spark-operator)"
    echo "   âœ… EMR ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë° RBAC"
    echo ""
    echo "ğŸš€ ë‹¤ìŒ ë‹¨ê³„:"
    echo "1. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‘ì—… ì‹¤í–‰:"
    echo "   python3 submit_test_spark_job.py"
    echo ""
    echo "2. Kubernetes Spark ì• í”Œë¦¬ì¼€ì´ì…˜ í…ŒìŠ¤íŠ¸:"
    echo "   kubectl apply -f test-spark-pi.yaml"
    echo "   kubectl get sparkapplications -n emr"
    echo ""
    echo "3. ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ì‘ì—…:"
    echo "   python3 generate_test_logs.py 10000 \$(grep DATA_BUCKET .env | cut -d'=' -f2)"
    echo "   python3 submit_spark_job.py"
    echo ""
    echo "ğŸ“Š ëª¨ë‹ˆí„°ë§ ëª…ë ¹ì–´:"
    echo "   # Pod ìƒíƒœ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"
    echo "   kubectl get pods -n emr -w"
    echo ""
    echo "   # Spark ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœ"
    echo "   kubectl get sparkapplications -n emr"
    echo ""
    echo "   # ì‘ì—… ë¡œê·¸ í™•ì¸"
    echo "   kubectl logs -f [pod-name] -n emr"
    echo ""
    echo "   # EMR ì‘ì—… ìƒíƒœ (AWS CLI)"
    echo "   aws emr-containers list-job-runs --virtual-cluster-id $VIRTUAL_CLUSTER_ID"
    echo ""
    echo "ğŸ’¡ íŒ: 'kubectl get pods -n emr -w' ëª…ë ¹ìœ¼ë¡œ ì‹¤ì‹œê°„ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
}

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
main() {
    echo "EMR Virtual Cluster ì„¤ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
    echo "ì˜ˆìƒ ì†Œìš” ì‹œê°„: 5-10ë¶„"
    echo ""

    read -p "ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): " confirm

    if [[ ! $confirm =~ ^[Yy]$ ]]; then
        echo "ì„¤ì •ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤."
        exit 0
    fi

    # ë‹¨ê³„ë³„ ì‹¤í–‰
    check_prerequisites
    setup_eks_connection
    setup_kubernetes_resources
    install_spark_operator
    create_emr_virtual_cluster
    create_test_spark_app
    update_environment_info
    verify_setup
    show_completion_message
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@"